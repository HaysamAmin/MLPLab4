{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Tidying**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Haysam"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of the data engineering process is data cleaning and tidying. What is done in those two processes, is trying to make the data more readable, and complete. This makes much easier to analyze, visualize, and train the data.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Tidying**\n",
    "\n",
    "Making the data more organized, and readable is the result of applying data tidying. \n",
    "\n",
    "In this section two main pandas functions are used in data tidying those are `melt` and `pivot_table`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by taking a look at the below dataframe, which represents the income ranges based on religion. This is part of the PEW research, which is famous in the US for conducting pollings and surveys on citizens.\n",
    "\n",
    "When the following are satisfied:\n",
    "\n",
    "\n",
    "1. Each variable forms a column\n",
    "2. Each observation forms a row\n",
    "3. Each type of observational unit forms a table\n",
    "\n",
    "We can then say that our dataset is *tidy*.\n",
    "\n",
    "First we need to import pandas to read csv datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[181]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "# importing dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PEW Research Dataset**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Start by Importing the dataset into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            religion   <$10k   $10-20k  $20-30k  $30-40k   $40-50k  $50-75k\n",
      "0           Agnostic      27        34       60       81        76      137\n",
      "1            Atheist      12        27       37       52        35       70\n",
      "2           Buddhist      27        21       30       34        33       58\n",
      "3           Catholic     418       617      732      670       638     1116\n",
      "4  Dont know/refused      15        14       15       11        10       35 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# loding the CSV file into a pandas dataframe #\n",
    "pew_path = 'CSVs/pew-raw.csv' # setting the path to the CSV file\n",
    "pew_df = pd.read_csv(pew_path)\n",
    "#printing the first 5 rows of the dataframe\n",
    "print(pew_df.head() ,'\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################### Columns Name ####################\n",
      "Index(['religion', ' <$10k', ' $10-20k', '$20-30k', '$30-40k', ' $40-50k',\n",
      "       '$50-75k'],\n",
      "      dtype='object')\n",
      "#################### Data shape ####################\n",
      "(10, 7)\n"
     ]
    }
   ],
   "source": [
    "print('#'*20, 'Columns Name', '#' * 20) # viewing the columns names\n",
    "print(pew_df.columns)\n",
    "print('#'*20, 'Data shape', '#' * 20) # viewing the columns names\n",
    "print(pew_df.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Observe the dataset using the `loc`, `iloc`, `head`, or `tail` approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n",
      "            religion   <$10k   $10-20k  $20-30k  $30-40k   $40-50k  $50-75k\n",
      "2           Buddhist      27        21       30       34        33       58\n",
      "3           Catholic     418       617      732      670       638     1116\n",
      "4  Dont know/refused      15        14       15       11        10       35\n"
     ]
    }
   ],
   "source": [
    "print(pew_df.loc[0,' $10-20k'])  # Display row with index '$20-30k'\n",
    "print(pew_df.iloc[2:5]) # Display rows 2 to 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################### tail ####################\n",
      "                   religion   <$10k   $10-20k  $20-30k  $30-40k   $40-50k  \\\n",
      "5         Evangelical Prot      575       869     1064      982       881   \n",
      "6                    Hindu        1         9        7        9        11   \n",
      "7  Historically Black Prot      228       244      236      238       197   \n",
      "8         Jehovahs Witness       20        27       24       24        21   \n",
      "9                   Jewish       19        19       25       25        30   \n",
      "\n",
      "   $50-75k  \n",
      "5     1486  \n",
      "6       34  \n",
      "7      223  \n",
      "8       30  \n",
      "9       95  \n"
     ]
    }
   ],
   "source": [
    "print('#'*20, 'tail', '#' * 20)\n",
    "print(pew_df.tail())  # Display last 5 rows"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***What does not seem right in the above dataframe?***\n",
    "- Some column names contain leading spaces ' $10-20k'\n",
    "- The data is understandable to humans but not suitable for data processing.\n",
    "- Each variable must have its own column\n",
    "- Each observation must have its own row\n",
    "- Each value must have its own cell \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Try to make the column headers represent a variable not a value. For that, use the `melt` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            religion  income  count\n",
      "0           Agnostic   <$10k     27\n",
      "1            Atheist   <$10k     12\n",
      "2           Buddhist   <$10k     27\n",
      "3           Catholic   <$10k    418\n",
      "4  Dont know/refused   <$10k     15\n"
     ]
    }
   ],
   "source": [
    "df_tidy = pew_df.melt(id_vars=\"religion\", var_name=\"income\", value_name=\"count\")\n",
    "print(df_tidy.head())  # Display first 5 rows of the tidy dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Converted income range strings into numerical min_income and max_income values to enable numerical analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    religion    income  count  min_income  max_income\n",
      "0                   Agnostic     <$10k     27           0       10000\n",
      "1                    Atheist     <$10k     12           0       10000\n",
      "2                   Buddhist     <$10k     27           0       10000\n",
      "3                   Catholic     <$10k    418           0       10000\n",
      "4          Dont know/refused     <$10k     15           0       10000\n",
      "5          Evangelical Prot      <$10k    575           0       10000\n",
      "6                     Hindu      <$10k      1           0       10000\n",
      "7   Historically Black Prot      <$10k    228           0       10000\n",
      "8          Jehovahs Witness      <$10k     20           0       10000\n",
      "9                    Jewish      <$10k     19           0       10000\n",
      "10                  Agnostic   $10-20k     34       10000       20000\n",
      "11                   Atheist   $10-20k     27       10000       20000\n",
      "12                  Buddhist   $10-20k     21       10000       20000\n",
      "13                  Catholic   $10-20k    617       10000       20000\n",
      "14         Dont know/refused   $10-20k     14       10000       20000\n",
      "15         Evangelical Prot    $10-20k    869       10000       20000\n",
      "16                    Hindu    $10-20k      9       10000       20000\n",
      "17  Historically Black Prot    $10-20k    244       10000       20000\n",
      "18         Jehovahs Witness    $10-20k     27       10000       20000\n",
      "19                   Jewish    $10-20k     19       10000       20000\n",
      "20                  Agnostic   $20-30k     60       20000       30000\n",
      "21                   Atheist   $20-30k     37       20000       30000\n",
      "22                  Buddhist   $20-30k     30       20000       30000\n",
      "23                  Catholic   $20-30k    732       20000       30000\n",
      "24         Dont know/refused   $20-30k     15       20000       30000\n",
      "25         Evangelical Prot    $20-30k   1064       20000       30000\n",
      "26                    Hindu    $20-30k      7       20000       30000\n",
      "27  Historically Black Prot    $20-30k    236       20000       30000\n",
      "28         Jehovahs Witness    $20-30k     24       20000       30000\n",
      "29                   Jewish    $20-30k     25       20000       30000\n",
      "30                  Agnostic   $30-40k     81       30000       40000\n",
      "31                   Atheist   $30-40k     52       30000       40000\n",
      "32                  Buddhist   $30-40k     34       30000       40000\n",
      "33                  Catholic   $30-40k    670       30000       40000\n",
      "34         Dont know/refused   $30-40k     11       30000       40000\n",
      "35         Evangelical Prot    $30-40k    982       30000       40000\n",
      "36                    Hindu    $30-40k      9       30000       40000\n",
      "37  Historically Black Prot    $30-40k    238       30000       40000\n",
      "38         Jehovahs Witness    $30-40k     24       30000       40000\n",
      "39                   Jewish    $30-40k     25       30000       40000\n",
      "40                  Agnostic   $40-50k     76       40000       50000\n",
      "41                   Atheist   $40-50k     35       40000       50000\n",
      "42                  Buddhist   $40-50k     33       40000       50000\n",
      "43                  Catholic   $40-50k    638       40000       50000\n",
      "44         Dont know/refused   $40-50k     10       40000       50000\n",
      "45         Evangelical Prot    $40-50k    881       40000       50000\n",
      "46                    Hindu    $40-50k     11       40000       50000\n",
      "47  Historically Black Prot    $40-50k    197       40000       50000\n",
      "48         Jehovahs Witness    $40-50k     21       40000       50000\n",
      "49                   Jewish    $40-50k     30       40000       50000\n",
      "50                  Agnostic   $50-75k    137       50000       75000\n",
      "51                   Atheist   $50-75k     70       50000       75000\n",
      "52                  Buddhist   $50-75k     58       50000       75000\n",
      "53                  Catholic   $50-75k   1116       50000       75000\n",
      "54         Dont know/refused   $50-75k     35       50000       75000\n",
      "55         Evangelical Prot    $50-75k   1486       50000       75000\n",
      "56                    Hindu    $50-75k     34       50000       75000\n",
      "57  Historically Black Prot    $50-75k    223       50000       75000\n",
      "58         Jehovahs Witness    $50-75k     30       50000       75000\n",
      "59                   Jewish    $50-75k     95       50000       75000\n"
     ]
    }
   ],
   "source": [
    "# Function to extract min and max income values\n",
    "def parse_income_range(income_str):\n",
    "    income_str = income_str.strip()\n",
    "    if income_str.startswith('<$'):\n",
    "        return (0, 10000)\n",
    "    elif '-' in income_str:\n",
    "        parts = income_str.replace('$', '').replace('k', '').split('-')\n",
    "        return (int(parts[0]) * 1000, int(parts[1]) * 1000)\n",
    "    elif income_str.startswith('>$'):\n",
    "        return (100000, np.nan)  # optional case\n",
    "    else:\n",
    "        return (np.nan, np.nan)\n",
    "\n",
    "# Apply parsing to income column\n",
    "df_tidy[['min_income', 'max_income']] = df_tidy['income'].apply(\n",
    "    lambda x: pd.Series(parse_income_range(x))\n",
    ")\n",
    "\n",
    "# Show result\n",
    "print(df_tidy)  # Display rows 5 to 9 of the tidy dataframe with income ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            religion  count  min_income  max_income\n",
      "0           Agnostic     27           0       10000\n",
      "1            Atheist     12           0       10000\n",
      "2           Buddhist     27           0       10000\n",
      "3           Catholic    418           0       10000\n",
      "4  Dont know/refused     15           0       10000 \n",
      "\n",
      "#################### Data types ####################\n",
      "checking the data types of the columns in the tidy dataframe\n",
      "religion      object\n",
      "count          int64\n",
      "min_income     int64\n",
      "max_income     int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_tidy.drop(columns='income', inplace=True) # Drop the original income column\n",
    "print(df_tidy.head(),'\\n')  # Display first 5 rows of the tidy dataframe after dropping the income column\n",
    "print('#'*20, 'Data types', '#' * 20)  # Displaying the data types of the columns in the tidy dataframe\n",
    "print('checking the data types of the columns in the tidy dataframe')\n",
    "print(df_tidy.dtypes)  # Display the data types of the columns in the tidy dataframe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Billboard Dataset**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset outlines data about the top hit songs on the Billboard list and the week from entrance that it was in the billboard with the ranking."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read the dataset and store it in a pandas dataframe. Note that the usual utf-8 encoding does not work on this dataset. The reason behind this is that there might be characters that are not supported by `utf-8`.\n",
    "\n",
    "The suggestion is to use for this dataset `unicode_escape` encoding. (converts all non-ASCII characters into their \\uXXXX representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading billboard.csv file using unicode_escape encoding\n",
    "billboard_path = 'CSVs/billboard.csv'  # setting the path to the CSV file\n",
    "billboard_df = pd.read_csv(billboard_path, encoding='unicode_escape')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Observe the first few rows of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe Shape:  (317, 83)\n",
      "#################### Columns Name ####################\n",
      "Index(['year', 'artist.inverted', 'track', 'time', 'genre', 'date.entered',\n",
      "       'date.peaked', 'x1st.week', 'x2nd.week', 'x3rd.week', 'x4th.week',\n",
      "       'x5th.week', 'x6th.week', 'x7th.week', 'x8th.week', 'x9th.week',\n",
      "       'x10th.week', 'x11th.week', 'x12th.week', 'x13th.week', 'x14th.week',\n",
      "       'x15th.week', 'x16th.week', 'x17th.week', 'x18th.week', 'x19th.week',\n",
      "       'x20th.week', 'x21st.week', 'x22nd.week', 'x23rd.week', 'x24th.week',\n",
      "       'x25th.week', 'x26th.week', 'x27th.week', 'x28th.week', 'x29th.week',\n",
      "       'x30th.week', 'x31st.week', 'x32nd.week', 'x33rd.week', 'x34th.week',\n",
      "       'x35th.week', 'x36th.week', 'x37th.week', 'x38th.week', 'x39th.week',\n",
      "       'x40th.week', 'x41st.week', 'x42nd.week', 'x43rd.week', 'x44th.week',\n",
      "       'x45th.week', 'x46th.week', 'x47th.week', 'x48th.week', 'x49th.week',\n",
      "       'x50th.week', 'x51st.week', 'x52nd.week', 'x53rd.week', 'x54th.week',\n",
      "       'x55th.week', 'x56th.week', 'x57th.week', 'x58th.week', 'x59th.week',\n",
      "       'x60th.week', 'x61st.week', 'x62nd.week', 'x63rd.week', 'x64th.week',\n",
      "       'x65th.week', 'x66th.week', 'x67th.week', 'x68th.week', 'x69th.week',\n",
      "       'x70th.week', 'x71st.week', 'x72nd.week', 'x73rd.week', 'x74th.week',\n",
      "       'x75th.week', 'x76th.week'],\n",
      "      dtype='object')\n",
      "#################### Billboard DataFrame ####################\n",
      "   year  artist.inverted                     track  time genre date.entered  \\\n",
      "0  2000  Destiny's Child  Independent Women Part I  3:38  Rock   2000-09-23   \n",
      "\n",
      "  date.peaked  x1st.week  x2nd.week  x3rd.week  ...  x67th.week  x68th.week  \\\n",
      "0  2000-11-18         78       63.0       49.0  ...         NaN         NaN   \n",
      "\n",
      "   x69th.week  x70th.week  x71st.week  x72nd.week  x73rd.week  x74th.week  \\\n",
      "0         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "\n",
      "   x75th.week  x76th.week  \n",
      "0         NaN         NaN  \n",
      "\n",
      "[1 rows x 83 columns]\n"
     ]
    }
   ],
   "source": [
    "print('Dataframe Shape: ', billboard_df.shape) # Displaying the shape of the billboard dataframe\n",
    "print('#'*20, 'Columns Name', '#' * 20)  # Displaying the columns names of the billboard dataframe  \n",
    "print(billboard_df.columns)  # Displaying the columns names of the billboard dataframe\n",
    "print('#'*20, 'Billboard DataFrame', '#' * 20)  # Displaying the billboard dataframe\n",
    "print(billboard_df.head(1))  # Display first 5 rows of the billboard dataframe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***What is wrong with the above dataset?***\n",
    "- Too many columns as there is 76 week columns per song it makes it hard to filter or summarize.\n",
    "- Time-Series Plots are hard as Weeks should be rows, not columns, for plotting rank over time.\n",
    "- Grouping is difficult. Can't easily calculate chart duration, peak rank, or average rank."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Let's, again, use the `melt` function to fix the general structure of the dataframe.\n",
    "\n",
    " **to reshape into long format, one row per song per week**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################### Long Format DataFrame ####################\n",
      "   year      artist.inverted                                  track  time  \\\n",
      "0  2000      Destiny's Child               Independent Women Part I  3:38   \n",
      "1  2000              Santana                           Maria, Maria  4:18   \n",
      "2  2000        Savage Garden                     I Knew I Loved You  4:07   \n",
      "3  2000              Madonna                                  Music  3:45   \n",
      "4  2000  Aguilera, Christina  Come On Over Baby (All I Want Is You)  3:38   \n",
      "\n",
      "  genre date.entered date.peaked       week  rank  \n",
      "0  Rock   2000-09-23  2000-11-18  x1st.week  78.0  \n",
      "1  Rock   2000-02-12  2000-04-08  x1st.week  15.0  \n",
      "2  Rock   1999-10-23  2000-01-29  x1st.week  71.0  \n",
      "3  Rock   2000-08-12  2000-09-16  x1st.week  41.0  \n",
      "4  Rock   2000-08-05  2000-10-14  x1st.week  57.0  \n",
      "Dataframe Shape:  (24092, 9)\n",
      "#################### Columns Name ####################\n",
      "Index(['year', 'artist.inverted', 'track', 'time', 'genre', 'date.entered',\n",
      "       'date.peaked', 'week', 'rank'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "week_cols = [col for col in billboard_df.columns if 'week' in col]\n",
    "\n",
    "df_long = pd.melt(\n",
    "    billboard_df,\n",
    "    id_vars=['year', 'artist.inverted', 'track', 'time', 'genre', 'date.entered', 'date.peaked'],\n",
    "    value_vars=week_cols,\n",
    "    var_name='week',\n",
    "    value_name='rank'\n",
    ")\n",
    "\n",
    "print('#'*20, 'Long Format DataFrame', '#' * 20)  # Displaying the long format dataframe\n",
    "print(df_long.head())  # Display first 10 rows of the long format dataframe\n",
    "# Displaying the shape of the long format dataframe\n",
    "print('Dataframe Shape: ', df_long.shape)\n",
    "# displaying the columns names of the long format dataframe\n",
    "print('#'*20, 'Columns Name', '#' * 20)\n",
    "print(df_long.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we inspect the current dataframe. We find that it is structured in a better way than before. \n",
    "\n",
    "However, the ***Week*** column looks a bit ugly!\n",
    "\n",
    "4. Let's try to place only the week number in that column without the extras surronding it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################### After Extracting Week ####################\n",
      "   year      artist.inverted                                  track  time  \\\n",
      "0  2000      Destiny's Child               Independent Women Part I  3:38   \n",
      "1  2000              Santana                           Maria, Maria  4:18   \n",
      "2  2000        Savage Garden                     I Knew I Loved You  4:07   \n",
      "3  2000              Madonna                                  Music  3:45   \n",
      "4  2000  Aguilera, Christina  Come On Over Baby (All I Want Is You)  3:38   \n",
      "5  2000                Janet                  Doesn't Really Matter  4:17   \n",
      "6  2000      Destiny's Child                            Say My Name  4:31   \n",
      "7  2000    Iglesias, Enrique                            Be With You  3:36   \n",
      "8  2000                Sisqo                             Incomplete  3:52   \n",
      "9  2000             Lonestar                                 Amazed  4:25   \n",
      "\n",
      "     genre date.entered date.peaked  week  rank  \n",
      "0     Rock   2000-09-23  2000-11-18     1  78.0  \n",
      "1     Rock   2000-02-12  2000-04-08     1  15.0  \n",
      "2     Rock   1999-10-23  2000-01-29     1  71.0  \n",
      "3     Rock   2000-08-12  2000-09-16     1  41.0  \n",
      "4     Rock   2000-08-05  2000-10-14     1  57.0  \n",
      "5     Rock   2000-06-17  2000-08-26     1  59.0  \n",
      "6     Rock   1999-12-25  2000-03-18     1  83.0  \n",
      "7    Latin   2000-04-01  2000-06-24     1  63.0  \n",
      "8     Rock   2000-06-24  2000-08-12     1  77.0  \n",
      "9  Country   1999-06-05  2000-03-04     1  81.0  \n"
     ]
    }
   ],
   "source": [
    "# Clean the 'week' column by extracting only the numeric week number using regex\n",
    "df_long['week'] = df_long['week'].str.extract(r'(\\d+)').astype(int)\n",
    "print('#' * 20, 'After Extracting Week', '#' * 20)\n",
    "print(df_long.head(10))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Now let's inspect the ***Week*** column in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################### Unique Values in Week Column ####################\n",
      "Number of unique weeks: 76\n",
      "#################### Week Column Description ####################\n",
      "count    24092.000000\n",
      "mean        38.500000\n",
      "std         21.937866\n",
      "min          1.000000\n",
      "25%         19.750000\n",
      "50%         38.500000\n",
      "75%         57.250000\n",
      "max         76.000000\n",
      "Name: week, dtype: float64\n",
      "#################### Week Column Unique Values ####################\n",
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76]\n"
     ]
    }
   ],
   "source": [
    "print('#' * 20, 'Unique Values in Week Column', '#' * 20)\n",
    "print(\"Number of unique weeks:\", df_long['week'].nunique())\n",
    "print('#' * 20, 'Week Column Description', '#' * 20)\n",
    "print(df_long['week'].describe())\n",
    "print('#' * 20, 'Week Column Unique Values', '#' * 20)\n",
    "print(df_long['week'].unique())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's try to find the date at which the song ranked the number that is shown per row.\n",
    "\n",
    "6. To do that let's first think of the equation that is going to get us the relevant date at which the song ranked the *rth*.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the actual date a song held a specific rank in the *r-th* week, use:  \n",
    "\n",
    "\n",
    " **$chart\\_date = date.entered + (week - 1) \\times 7\\ \\text{days}$**\n",
    "\n",
    "\n",
    "- week - 1: because the 1st week is the start date itself  \n",
    "- Multiply by 7 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year      artist.inverted                                  track  time  \\\n",
      "0  2000      Destiny's Child               Independent Women Part I  3:38   \n",
      "1  2000              Santana                           Maria, Maria  4:18   \n",
      "2  2000        Savage Garden                     I Knew I Loved You  4:07   \n",
      "3  2000              Madonna                                  Music  3:45   \n",
      "4  2000  Aguilera, Christina  Come On Over Baby (All I Want Is You)  3:38   \n",
      "\n",
      "  genre date.entered date.peaked  week  rank chart_date  \n",
      "0  Rock   2000-09-23  2000-11-18     1  78.0 2000-09-23  \n",
      "1  Rock   2000-02-12  2000-04-08     1  15.0 2000-02-12  \n",
      "2  Rock   1999-10-23  2000-01-29     1  71.0 1999-10-23  \n",
      "3  Rock   2000-08-12  2000-09-16     1  41.0 2000-08-12  \n",
      "4  Rock   2000-08-05  2000-10-14     1  57.0 2000-08-05  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convert date.entered to datetime\n",
    "df_long['date.entered'] = pd.to_datetime(df_long['date.entered'])\n",
    "\n",
    "# Calculate chart date using timedelta\n",
    "df_long['chart_date'] = df_long['date.entered'] + pd.to_timedelta(df_long['week'] - 1, unit='W')\n",
    "\n",
    "print(df_long.head())  # Display first  rows of the long format dataframe with chart_date"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Timedeltas are absolute differences in times, expressed in difference units (e.g. days, hours, minutes, seconds). This method converts an argument from a recognized timedelta format / value into a Timedelta type.*\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***What is the problem with the calculation above?***\n",
    "\n",
    "1- Assumes fixed 7-Day Intervals. May not align with the actual Billboard chart release schedule (e.g., always on Saturdays).\n",
    "\n",
    "2- Depends on accurate date.entered. If date.entered is missing or invalid, the calculation fails.\n",
    "\n",
    "3- Requires Valid Numeric week Values. If week is not an integer or contains NaN, the calculation may raise an error or return incorrect results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Let's only keep necessary columns\n",
    "\n",
    "- To keep only the necessary columns for analysis, we can create a subset of the DataFrame with just the important ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   track      artist.inverted  week  rank  \\\n",
      "0               Independent Women Part I      Destiny's Child     1  78.0   \n",
      "1                           Maria, Maria              Santana     1  15.0   \n",
      "2                     I Knew I Loved You        Savage Garden     1  71.0   \n",
      "3                                  Music              Madonna     1  41.0   \n",
      "4  Come On Over Baby (All I Want Is You)  Aguilera, Christina     1  57.0   \n",
      "\n",
      "  chart_date  \n",
      "0 2000-09-23  \n",
      "1 2000-02-12  \n",
      "2 1999-10-23  \n",
      "3 2000-08-12  \n",
      "4 2000-08-05  \n"
     ]
    }
   ],
   "source": [
    "# Keep only the relevant columns\n",
    "df_long = df_long[['track', 'artist.inverted', 'week', 'rank', 'chart_date']]\n",
    "\n",
    "# Preview the result\n",
    "print(df_long.head())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. How to rename your columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To rename columns using the .rename() method with a dictionary that maps old names to new ones.\n",
    "\n",
    "df_long = df_long.rename(columns={\n",
    "    'track': 'Track',\n",
    "    'artist.inverted': 'Artist',\n",
    "    'week': 'Week',\n",
    "    'rank': 'Rank',\n",
    "    'chart_date': 'ChartDate'\n",
    "})\n",
    "# Display the first 5 rows of the renamed dataframe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################### Renamed DataFrame ####################\n",
      "                                       Track               Artist  Week  Rank  \\\n",
      "0                   Independent Women Part I      Destiny's Child     1  78.0   \n",
      "1                               Maria, Maria              Santana     1  15.0   \n",
      "2                         I Knew I Loved You        Savage Garden     1  71.0   \n",
      "3                                      Music              Madonna     1  41.0   \n",
      "4      Come On Over Baby (All I Want Is You)  Aguilera, Christina     1  57.0   \n",
      "...                                      ...                  ...   ...   ...   \n",
      "24087                       Cherchez LaGhost     Ghostface Killah    76   NaN   \n",
      "24088                            Freakin' It          Smith, Will    76   NaN   \n",
      "24089                          Kernkraft 400        Zombie Nation    76   NaN   \n",
      "24090                               Got Beef       Eastsidaz, The    76   NaN   \n",
      "24091                         Toca's Miracle               Fragma    76   NaN   \n",
      "\n",
      "       ChartDate  \n",
      "0     2000-09-23  \n",
      "1     2000-02-12  \n",
      "2     1999-10-23  \n",
      "3     2000-08-12  \n",
      "4     2000-08-05  \n",
      "...          ...  \n",
      "24087 2002-01-12  \n",
      "24088 2001-07-21  \n",
      "24089 2002-02-09  \n",
      "24090 2001-12-08  \n",
      "24091 2002-04-06  \n",
      "\n",
      "[24092 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "print('#' * 20, 'Renamed DataFrame', '#' * 20)\n",
    "# Display the updated dataframe\n",
    "print(df_long)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above dataframe, there are some *NaN* values. What are we going to do? \n",
    "\n",
    "option 1: Drop NaN and Keep only charted weeks\n",
    "\n",
    "\n",
    "option 2: Fill NaN with placeholder. Mark off-chart weeks (e.g., 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long = df_long.dropna(subset=['Rank'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Apply quick data cleaning and then observe the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   Track               Artist  Week  Rank  \\\n",
      "0               Independent Women Part I      Destiny's Child     1  78.0   \n",
      "1                           Maria, Maria              Santana     1  15.0   \n",
      "2                     I Knew I Loved You        Savage Garden     1  71.0   \n",
      "3                                  Music              Madonna     1  41.0   \n",
      "4  Come On Over Baby (All I Want Is You)  Aguilera, Christina     1  57.0   \n",
      "\n",
      "   ChartDate  \n",
      "0 2000-09-23  \n",
      "1 2000-02-12  \n",
      "2 1999-10-23  \n",
      "3 2000-08-12  \n",
      "4 2000-08-05  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Reset the index\n",
    "df_long.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the cleaned DataFrame\n",
    "print(df_long.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Cleaning**\n",
    "\n",
    "Data cleaning involves removing unwanted characters, imputing, or dropping missing values.\n",
    "\n",
    "The decision is based on the dataset you have, and the information you can extract from the other columns.\n",
    "\n",
    "\n",
    "Examples of data cleaning include cleaning:\n",
    "\n",
    "1.   **Missing Data**\n",
    "2.   **Irregular Data** (Outliers)\n",
    "3.   **Unnecessary Data** — Repetitive Data, Duplicates and more\n",
    "4.   **Inconsistent Data** — Capitalization, Addresses and more\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cars Data Set**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by reading the dataset related to car models: ./CSVs/cars.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################### Cars DataFrame ####################\n",
      "                         Car     MPG Cylinders Displacement Horsepower  \\\n",
      "0                     STRING  DOUBLE       INT       DOUBLE     DOUBLE   \n",
      "1  Chevrolet Chevelle Malibu     NaN         8        307.0      130.0   \n",
      "2          Buick Skylark 320    15.0         8        350.0        NaN   \n",
      "3         Plymouth Satellite     NaN         8        318.0      150.0   \n",
      "4              AMC Rebel SST    16.0         8          NaN      150.0   \n",
      "\n",
      "   Weight Acceleration Model Origin  \n",
      "0  DOUBLE       DOUBLE   INT    CAT  \n",
      "1   3504.         12.0    70     US  \n",
      "2   3693.         11.5    70     US  \n",
      "3   3436.         11.0    70     US  \n",
      "4     NaN         12.0    70     US  \n"
     ]
    }
   ],
   "source": [
    "# Read the dataset with proper delimiter\n",
    "car_path = './CSVs/cars.csv'  # setting the path to the CSV file\n",
    "cars_df = pd.read_csv(car_path,sep=';')  # Load the CSV file into a DataFrame\n",
    "# sep=';' specifies that the delimiter is a semicolon\n",
    "print('#' * 20, 'Cars DataFrame', '#' * 20)  # Displaying the cars dataframe\n",
    "print(cars_df.head())  # Display first 5 rows of the cars dataframe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First Row seems to be the datatype, we need to remove it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Car   MPG Cylinders Displacement Horsepower Weight  \\\n",
      "0  Chevrolet Chevelle Malibu   NaN         8        307.0      130.0  3504.   \n",
      "1          Buick Skylark 320  15.0         8        350.0        NaN  3693.   \n",
      "2         Plymouth Satellite   NaN         8        318.0      150.0  3436.   \n",
      "3              AMC Rebel SST  16.0         8          NaN      150.0    NaN   \n",
      "4                Ford Torino  17.0         8        302.0      140.0  3449.   \n",
      "\n",
      "  Acceleration Model Origin  \n",
      "0         12.0    70     US  \n",
      "1         11.5    70     US  \n",
      "2         11.0    70     US  \n",
      "3         12.0    70     US  \n",
      "4         10.5    70     US  \n"
     ]
    }
   ],
   "source": [
    "# Read the dataset with semicolon separator\n",
    "cars_df = pd.read_csv('./CSVs/cars.csv', sep=';')\n",
    "\n",
    "# Drop the first row (which contains data types like STRING, DOUBLE, etc.)\n",
    "cars_df = cars_df.drop(index=0).reset_index(drop=True)\n",
    "\n",
    "# Preview the cleaned dataset\n",
    "print(cars_df.head())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's observe the columns with null values.  Either by using the `isnull().sum()` function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Car             0\n",
      "MPG             3\n",
      "Cylinders       0\n",
      "Displacement    1\n",
      "Horsepower      2\n",
      "Weight          1\n",
      "Acceleration    0\n",
      "Model           0\n",
      "Origin          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Display the number of null values in each column\n",
    "print(cars_df.isnull().sum())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There aren't many missing values. Let's take a glimpse at the percentage of the missing values:\n",
    "\n",
    "**HINT:** We'll need `Numpy` for the below task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################### Missing Values in each column ####################\n",
      "Car             0.00\n",
      "MPG             0.74\n",
      "Cylinders       0.00\n",
      "Displacement    0.25\n",
      "Horsepower      0.49\n",
      "Weight          0.25\n",
      "Acceleration    0.00\n",
      "Model           0.00\n",
      "Origin          0.00\n",
      "dtype: float64\n",
      "#################### Missing Values Percentage ####################\n",
      "The percentage of missing values in the dataset is: 1.72%\n"
     ]
    }
   ],
   "source": [
    "# Calculate the percentage of missing values for each column\n",
    "missing_percentage = (cars_df.isnull().sum() / len(cars_df)) * 100\n",
    "\n",
    "# Round and display\n",
    "print('#' * 20, 'Missing Values in each column', '#' * 20)\n",
    "print(np.round(missing_percentage, 2))\n",
    "print('#' * 20, 'Missing Values Percentage', '#' * 20)\n",
    "missing_percentage = (cars_df.isnull().sum().sum() / len(cars_df)) * 100\n",
    "print(f'The percentage of missing values in the dataset is: {missing_percentage:.2f}%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Around *0.19%* of the values are missing, which isn't a lot. Therefore, we might go with the option of dropping all the rows with null values."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets also check dropping the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining columns after dropping those with missing values:\n",
      "Index(['Car', 'Cylinders', 'Acceleration', 'Model', 'Origin'], dtype='object')\n",
      "                         Car Cylinders Acceleration Model Origin\n",
      "0  Chevrolet Chevelle Malibu         8         12.0    70     US\n",
      "1          Buick Skylark 320         8         11.5    70     US\n",
      "2         Plymouth Satellite         8         11.0    70     US\n",
      "3              AMC Rebel SST         8         12.0    70     US\n",
      "4                Ford Torino         8         10.5    70     US\n"
     ]
    }
   ],
   "source": [
    "# Drop columns that contain any missing values\n",
    "cars_dropped_cols = cars_df.dropna(axis=1)\n",
    "\n",
    "# Display remaining columns\n",
    "print(\"Remaining columns after dropping those with missing values:\")\n",
    "print(cars_dropped_cols.columns)\n",
    "\n",
    "# Preview the result\n",
    "print(cars_dropped_cols.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's observe how many columns we lost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns dropped due to missing values: 4 \n",
      "\n",
      "Columns dropped due to missing values:\n",
      "{'Horsepower', 'Displacement', 'MPG', 'Weight'}\n"
     ]
    }
   ],
   "source": [
    "# Count how many columns were dropped\n",
    "dropped_columns = cars_df.shape[1] - cars_dropped_cols.shape[1]\n",
    "\n",
    "print(f\"Number of columns dropped due to missing values: {dropped_columns}\", '\\n')\n",
    "\n",
    "# Identify columns that were dropped\n",
    "dropped_cols = set(cars_df.columns) - set(cars_dropped_cols.columns)\n",
    "\n",
    "print(\"Columns dropped due to missing values:\")\n",
    "print(dropped_cols)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cars Dataset - Filling in missing values automatically**\n",
    "\n",
    "Another option is to try and fill in the missing values through imputations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take the `MPG` column for example. We can fill in the missing values with 0s through the following line of code:\n",
    "\n",
    "`df_cars.fillna(0) `. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values in the 'MPG' column with 0\n",
    "cars_df['MPG'] = cars_df['MPG'].fillna(0)\n",
    "\n",
    "# Preview the result\n",
    "print(cars_df['MPG'].head())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this does not make much sense as there isn't MPG equal to 0. How about we plot the MPG column and if it follows a random distribution we can use the mean of the column to compute the missing values. Otherwise, we can use the median (if there is a skewed normal distribution). However, there might be a better way of imputation which is getting the median or the mean of the MPG of the cars with similar attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram and KDE of MPG\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.histplot(cars_df['MPG'].dropna(), kde=True, bins=30, color='skyblue')\n",
    "plt.title('Distribution of MPG')\n",
    "plt.xlabel('MPG')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we observe the graph above, we can consider it in a way or another normally distributed. Therefore, we can impute the missing values using the mean."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the mean we need numeric values. However the values in the dataframe are objects. Therefore, we need to change them to numerics so that we can compute them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what is the mean of the MPG column"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this mean to compute the missing values since the graph demonstarted a normal distribution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Car Dataset - Simple Imputer**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*SimpleImputer* is a `scikit-learn` class which is helpful in handling the missing data in the predictive model dataset. It replaces the `NaN` values with a specified placeholder.\n",
    "It is implemented by the use of the `SimpleImputer()` method which takes the following arguments :\n",
    "\n",
    "`missing_values` : The missing_values placeholder which has to be imputed. By default is NaN\n",
    "\n",
    "`strategy` : The data which will replace the NaN values from the dataset. The strategy argument can take the values – ‘mean'(default), ‘median’, ‘most_frequent’ and ‘constant’.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing the `SimpleImputer` into our notebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we need to do are two essential steps:\n",
    "\n",
    "1. fit the data (compute the mean / median / most freq)\n",
    "2. transform the data (place the computed values in the NaN cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Outlier Detection** \n",
    "\n",
    "\n",
    "An Outlier is a data-item/object that deviates significantly from the rest of the (so-called normal)objects. They can be caused by measurement or execution errors. The analysis for outlier detection is referred to as outlier mining. There are many ways to detect the outliers, and the removal process is the data frame same as removing a data item from the panda’s data frame.\n",
    "\n",
    "\n",
    "\n",
    "https://www.geeksforgeeks.org/detect-and-remove-the-outliers-using-python/\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing Outliers Using Box Plot\n",
    "It captures the summary of the data effectively and efficiently with only a simple box and whiskers. Boxplot summarizes sample data using 25th, 50th, and 75th percentiles. One can just get insights(quartiles, median, and outliers) into the dataset by just looking at its boxplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing Outliers Using ScatterPlot.\n",
    "\n",
    "It is used when you have paired numerical data and when your dependent variable has multiple values for each reading independent variable, or when trying to determine the relationship between the two variables. In the process of utilizing the scatter plot, one can also use it for outlier detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Z-Score:\n",
    "Z- Score is also called a standard score. This value/score helps to understand that how far is the data point from the mean. And after setting up a threshold value one can utilize z score values of data points to define the outliers.\n",
    "<br>\n",
    "Zscore = (data_point -mean) / std. deviation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to define an outlier threshold value is chosen which is generally 3.0. As 99.7% of the data points lie between +/- 3 standard deviation (using Gaussian Distribution approach).\n",
    "\n",
    "Rows where Z value is greater than 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IQR (Inter-Quartile Range)\n",
    "Inter Quartile Range approach to finding the outliers is the most commonly used and most trusted approach used in the research field. <Br>\n",
    "IQR = Quartile3 - Quartile1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define the outlier base value is defined above and below dataset’s normal range namely Upper and Lower bounds, define the upper and the lower bound (1.5*IQR value is considered) :<br>\n",
    "upper = Q3 + 1.5 * IQR <br>\n",
    "lower = Q1 - 1.5 * IQR <br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Removing the outliers:\n",
    "For removing the outlier, one must follow the same process of removing an entry from the dataset using its exact position in the dataset because in all the above methods of detecting the outliers end result is the list of all those data items that satisfy the outlier definition according to the method used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
